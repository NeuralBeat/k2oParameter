{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DEPENDENCIES AND LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import knock_evaluator\n",
    "\n",
    "from knock_evaluator import mimic_knock_detection, find_knock_event_ended, calculate_ZImpactReturn, calculate_Very_High_Impact, calculate_DeployFlag, reshape_sequence, calculate_Has_It_Knocked\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, label):\n",
    "    \"\"\"\n",
    "    Process all files in the given folder and compile data into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing the files.\n",
    "    - label: The label to assign to all data from this folder (e.g., 1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing all processed data from the files.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to hold data from all files\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure we're only processing .csv files\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.csv'):\n",
    "            # Read the file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Filter rows for xBuffer, yBuffer, zBuffer and reset index\n",
    "            x_data = data[data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "            y_data = data[data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "            z_data = data[data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "            \n",
    "            # Ensure data is aligned\n",
    "            min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "            structured_df = pd.DataFrame({\n",
    "                'x': x_data['Value'].head(min_length),\n",
    "                'y': y_data['Value'].head(min_length),\n",
    "                'z': z_data['Value'].head(min_length),\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            # Append to the list\n",
    "            all_data.append(structured_df)\n",
    "    \n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Expression</th>\n",
       "      <th>Value</th>\n",
       "      <th>Location</th>\n",
       "      <th>Refresh</th>\n",
       "      <th>Access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>extractionBuffer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xBuffer</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>yBuffer</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0x20005EDE</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>zBuffer</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0x20005EE0</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level        Expression  Value    Location Refresh   Access\n",
       "0      0  extractionBuffer    NaN  0x20005EDC     Off  private\n",
       "1      1               [0]    NaN  0x20005EDC     Off   public\n",
       "2      2           xBuffer    9.0  0x20005EDC     Off   public\n",
       "3      2           yBuffer  116.0  0x20005EDE     Off   public\n",
       "4      2           zBuffer   54.0  0x20005EE0     Off   public"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the valid knock data\n",
    "valid_data_path = 'data/valid/valid1.csv'\n",
    "valid_data = pd.read_csv(valid_data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y     z  label\n",
       "0    9.0  116.0  54.0      1\n",
       "1    2.0  -20.0  99.0      1\n",
       "2  100.0  111.0 -22.0      1\n",
       "3   33.0   76.0  34.0      1\n",
       "4   15.0  -25.0 -24.0      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract only the rows that contain acceleration values and reset index to avoid grouping issues\n",
    "x_data = valid_data[valid_data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "y_data = valid_data[valid_data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "z_data = valid_data[valid_data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "\n",
    "# Ensure we only take as many rows as the shortest among x, y, z to keep data aligned\n",
    "min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "\n",
    "# Reconstruct the DataFrame using the aligned data\n",
    "structured_df_aligned = pd.DataFrame({\n",
    "    'x': x_data['Value'].head(min_length),\n",
    "    'y': y_data['Value'].head(min_length),\n",
    "    'z': z_data['Value'].head(min_length),\n",
    "    'label': 1  # Label for valid knock\n",
    "})\n",
    "\n",
    "structured_df_aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_sequences(df, label):\n",
    "    \"\"\"\n",
    "    Structures the DataFrame such that each 90-row sequence (representing 30 time points of x, y, z data)\n",
    "    is treated as a single observation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the sequences.\n",
    "    - label: The label for these sequences (1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tuples, where each tuple is (sequence, label), and\n",
    "      each sequence is a (90, ) shape array if flattening or a (30, 3) array if keeping x, y, z separate.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    num_sequences = len(df) // 90  # Assuming each sequence is exactly 90 rows\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start_idx = i * 90\n",
    "        sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.flatten()  # Flattened sequence\n",
    "        # Alternatively, keep as a (30, 3) array for models that can handle sequence data\n",
    "        # sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.reshape((30, 3))\n",
    "        sequences.append((sequence, label))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all valid / invalid data into data frames, Label & Combine them to Sequences. Afterwards Combine them to global Sequence structure and Shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_valid = 'data/valid'  \n",
    "folder_path_invalid = 'data/invalid'  \n",
    "valid_data = process_folder(folder_path_valid, label=1)\n",
    "invalid_data = process_folder(folder_path_invalid, label=0)\n",
    "\n",
    "# Assuming valid_data and invalid_data are already loaded and structured with one file per sequence\n",
    "valid_sequences = structure_sequences(valid_data, 1)\n",
    "invalid_sequences = structure_sequences(invalid_data, 0)\n",
    "\n",
    "all_sequences = valid_sequences + invalid_sequences\n",
    "random.shuffle(all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for model training\n",
    "X = np.array([seq[0] for seq in all_sequences])\n",
    "y = np.array([seq[1] for seq in all_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Test Split of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': [10, 15, 20, 25, 30],\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': [10, 15, 20, 25, 30],\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': [3, 5, 7, 9],\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': [6000, 7500, 8500, 9500], \n",
    "    'KLOPFALGO_VelThrZLatched_s32': [7000, 8000, 9000 ,10000],\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': [4,6,8,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = reshape_sequence(all_sequences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuffer_size = 8\n",
    "knock_event_length_avg = 65\n",
    "knock_event_ended = prebuffer_size + knock_event_length_avg\n",
    "\n",
    "knock_algo_Vel_Zlatched = 9000\n",
    "knock_algo_Vel_Zlatched_Counter = 20\n",
    "knock_algo_HFA_Zlatched = 3200\n",
    "knock_algo_HFA_Zlatched_Counter = 15\n",
    "ZImpactReturn = False\n",
    "Very_High_Impact = False\n",
    "DeployFlag = False\n",
    "\n",
    "Has_It_Knocked = False\n",
    "\n",
    "very_high_counter = 5\n",
    "Acc_Latched_Counter = 25\n",
    "Acc_Latched_Threshold = 8500\n",
    "Vel_Latched_Counter = 30\n",
    "Vel_Latched_Threshold = 9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Quotient Z/X is big enough:  False  ---> For positive Recognition required: True\n",
      "Quotient Z/Y is big enough:  True  ---> For positive Recognition required: True\n",
      "ZImpactReturn:  False               ---> For positive Recognition required: True\n",
      "Very_High_Acc_Impact:  False        ---> For positive Recognition required: False\n",
      "Very_High_Vel_Impact:  False        ---> For positive Recognition required: False\n",
      "Very_High_Impact:  False            ---> For positive Recognition required: False\n",
      "Acc_Latched_Counter:  27            ---> For positive Recognition required: < 25\n",
      "Vel_Latched_Counter:  5             ---> For positive Recognition required: < 30\n",
      "DeployFlag:  False                  ---> For positive Recognition required: True\n"
     ]
    }
   ],
   "source": [
    "# Calculate the velocities as cumulative sum (integration) of each buffer\n",
    "sequence_df['xBuffer_quasi_velocity'] = np.cumsum(sequence_df['xBuffer'])\n",
    "sequence_df['yBuffer_quasi_velocity'] = np.cumsum(sequence_df['yBuffer'])\n",
    "sequence_df['zBuffer_quasi_velocity'] = np.cumsum(sequence_df['zBuffer'])\n",
    "\n",
    "# Calculate the energies of each buffer\n",
    "sequence_df['xBuffer_quasi_energy'] = (sequence_df['xBuffer_quasi_velocity'])**2\n",
    "sequence_df['yBuffer_quasi_energy'] = (sequence_df['yBuffer_quasi_velocity'])**2\n",
    "sequence_df['zBuffer_quasi_energy'] = (sequence_df['zBuffer_quasi_velocity'])**2\n",
    "\n",
    "sequence_df['xBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['xBuffer_quasi_velocity']))\n",
    "sequence_df['yBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['yBuffer_quasi_velocity']))\n",
    "sequence_df['zBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['zBuffer_quasi_velocity']))\n",
    "\n",
    "knock_event_ended_dynamic = find_knock_event_ended(sequence_df['zBuffer'], knock_algo_HFA_Zlatched, knock_algo_HFA_Zlatched_Counter, prebuffer_size)\n",
    "\n",
    "# Nulling values after the knock_event_ended parameter by setting them to 0\n",
    "# Update based on the dynamically determined knock_event_ended value\n",
    "if knock_event_ended_dynamic is not None and knock_event_ended_dynamic != 0:\n",
    "    knock_event_ended = knock_event_ended_dynamic\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "else:\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "\n",
    "#Calculate several keay values\n",
    "maxZ_Acc = np.abs(sequence_df['zBuffer']).max()\n",
    "maxX_Vel = np.abs(sequence_df['xBuffer_quasi_velocity']).max()\n",
    "maxY_Vel = np.abs(sequence_df['yBuffer_quasi_velocity']).max()\n",
    "maxZ_Vel = np.abs(sequence_df['zBuffer_quasi_velocity']).max()\n",
    "#minZ_Vel = pivoted_data['zBuffer_quasi_velocity'].min()\n",
    "divXZ = maxZ_Vel/maxX_Vel\n",
    "divYZ = maxZ_Vel/maxY_Vel\n",
    "\n",
    "ZImpactReturn = calculate_ZImpactReturn(maxZ_Vel, maxX_Vel, maxY_Vel)\n",
    "Very_High_Impact = calculate_Very_High_Impact(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], very_high_counter)\n",
    "DeployFlag = calculate_DeployFlag(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], prebuffer_size, knock_event_ended, Acc_Latched_Counter, Vel_Latched_Counter, Acc_Latched_Threshold, Vel_Latched_Threshold)\n",
    "Has_It_Knocked = calculate_Has_It_Knocked(DeployFlag[0], Very_High_Impact, ZImpactReturn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(Has_It_Knocked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
