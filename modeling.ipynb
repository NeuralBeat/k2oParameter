{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DEPENDENCIES AND LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import knock_evaluator\n",
    "\n",
    "from knock_evaluator import mimic_knock_detection, find_knock_event_ended, calculate_ZImpactReturn, calculate_Very_High_Impact, calculate_DeployFlag, reshape_sequence, calculate_Has_It_Knocked\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, label):\n",
    "    \"\"\"\n",
    "    Process all files in the given folder and compile data into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing the files.\n",
    "    - label: The label to assign to all data from this folder (e.g., 1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing all processed data from the files.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to hold data from all files\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure we're only processing .csv files\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.csv'):\n",
    "            # Read the file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Filter rows for xBuffer, yBuffer, zBuffer and reset index\n",
    "            x_data = data[data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "            y_data = data[data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "            z_data = data[data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "            \n",
    "            # Ensure data is aligned\n",
    "            min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "            structured_df = pd.DataFrame({\n",
    "                'x': x_data['Value'].head(min_length),\n",
    "                'y': y_data['Value'].head(min_length),\n",
    "                'z': z_data['Value'].head(min_length),\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            # Append to the list\n",
    "            all_data.append(structured_df)\n",
    "    \n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Expression</th>\n",
       "      <th>Value</th>\n",
       "      <th>Location</th>\n",
       "      <th>Refresh</th>\n",
       "      <th>Access</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>extractionBuffer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xBuffer</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0x20005EDC</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>yBuffer</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0x20005EDE</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>zBuffer</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0x20005EE0</td>\n",
       "      <td>Off</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level        Expression  Value    Location Refresh   Access\n",
       "0      0  extractionBuffer    NaN  0x20005EDC     Off  private\n",
       "1      1               [0]    NaN  0x20005EDC     Off   public\n",
       "2      2           xBuffer    9.0  0x20005EDC     Off   public\n",
       "3      2           yBuffer  116.0  0x20005EDE     Off   public\n",
       "4      2           zBuffer   54.0  0x20005EE0     Off   public"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the valid knock data\n",
    "valid_data_path = 'data/valid/valid1.csv'\n",
    "valid_data = pd.read_csv(valid_data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y     z  label\n",
       "0    9.0  116.0  54.0      1\n",
       "1    2.0  -20.0  99.0      1\n",
       "2  100.0  111.0 -22.0      1\n",
       "3   33.0   76.0  34.0      1\n",
       "4   15.0  -25.0 -24.0      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract only the rows that contain acceleration values and reset index to avoid grouping issues\n",
    "x_data = valid_data[valid_data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "y_data = valid_data[valid_data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "z_data = valid_data[valid_data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "\n",
    "# Ensure we only take as many rows as the shortest among x, y, z to keep data aligned\n",
    "min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "\n",
    "# Reconstruct the DataFrame using the aligned data\n",
    "structured_df_aligned = pd.DataFrame({\n",
    "    'x': x_data['Value'].head(min_length),\n",
    "    'y': y_data['Value'].head(min_length),\n",
    "    'z': z_data['Value'].head(min_length),\n",
    "    'label': 1  # Label for valid knock\n",
    "})\n",
    "\n",
    "structured_df_aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_sequences(df, label):\n",
    "    \"\"\"\n",
    "    Structures the DataFrame such that each 90-row sequence (representing 30 time points of x, y, z data)\n",
    "    is treated as a single observation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the sequences.\n",
    "    - label: The label for these sequences (1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tuples, where each tuple is (sequence, label), and\n",
    "      each sequence is a (90, ) shape array if flattening or a (30, 3) array if keeping x, y, z separate.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    num_sequences = len(df) // 90  # Assuming each sequence is exactly 90 rows\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start_idx = i * 90\n",
    "        sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.flatten()  # Flattened sequence\n",
    "        # Alternatively, keep as a (30, 3) array for models that can handle sequence data\n",
    "        # sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.reshape((30, 3))\n",
    "        sequences.append((sequence, label))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all valid / invalid data into data frames, Label & Combine them to Sequences. Afterwards Combine them to global Sequence structure and Shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_valid = 'data/valid'  \n",
    "folder_path_invalid = 'data/invalid'  \n",
    "valid_data = process_folder(folder_path_valid, label=1)\n",
    "invalid_data = process_folder(folder_path_invalid, label=0)\n",
    "\n",
    "# Assuming valid_data and invalid_data are already loaded and structured with one file per sequence\n",
    "valid_sequences = structure_sequences(valid_data, 1)\n",
    "invalid_sequences = structure_sequences(invalid_data, 0)\n",
    "\n",
    "all_sequences = valid_sequences + invalid_sequences\n",
    "random.shuffle(all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for model training\n",
    "X = np.array([seq[0] for seq in all_sequences])\n",
    "y = np.array([seq[1] for seq in all_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Test Split of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = reshape_sequence(all_sequences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuffer_size = 8\n",
    "knock_event_length_avg = 45\n",
    "knock_event_ended = prebuffer_size + knock_event_length_avg\n",
    "knock_algo_AngleXLatched = 8\n",
    "knock_algo_Vel_Zlatched = 9000\n",
    "knock_algo_Vel_Zlatched_Counter = 20\n",
    "knock_algo_HFA_Zlatched = 3200\n",
    "knock_algo_HFA_Zlatched_Counter = 15\n",
    "ZImpactReturn = False\n",
    "Very_High_Impact = False\n",
    "DeployFlag = False\n",
    "\n",
    "Has_It_Knocked = False\n",
    "\n",
    "very_high_counter = 5\n",
    "Acc_Latched_Counter = 25\n",
    "Acc_Latched_Threshold = 8500\n",
    "Vel_Latched_Counter = 30\n",
    "Vel_Latched_Threshold = 9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the velocities as cumulative sum (integration) of each buffer\n",
    "sequence_df['xBuffer_quasi_velocity'] = np.cumsum(sequence_df['xBuffer'])\n",
    "sequence_df['yBuffer_quasi_velocity'] = np.cumsum(sequence_df['yBuffer'])\n",
    "sequence_df['zBuffer_quasi_velocity'] = np.cumsum(sequence_df['zBuffer'])\n",
    "\n",
    "# Calculate the energies of each buffer\n",
    "sequence_df['xBuffer_quasi_energy'] = (sequence_df['xBuffer_quasi_velocity'])**2\n",
    "sequence_df['yBuffer_quasi_energy'] = (sequence_df['yBuffer_quasi_velocity'])**2\n",
    "sequence_df['zBuffer_quasi_energy'] = (sequence_df['zBuffer_quasi_velocity'])**2\n",
    "\n",
    "sequence_df['xBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['xBuffer_quasi_velocity']))\n",
    "sequence_df['yBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['yBuffer_quasi_velocity']))\n",
    "sequence_df['zBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['zBuffer_quasi_velocity']))\n",
    "\n",
    "knock_event_ended_dynamic = find_knock_event_ended(sequence_df['zBuffer'], knock_algo_HFA_Zlatched, knock_algo_HFA_Zlatched_Counter, prebuffer_size)\n",
    "\n",
    "# Nulling values after the knock_event_ended parameter by setting them to 0\n",
    "# Update based on the dynamically determined knock_event_ended value\n",
    "if knock_event_ended_dynamic is not None and knock_event_ended_dynamic != 0:\n",
    "    knock_event_ended = knock_event_ended_dynamic\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "else:\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "\n",
    "#Calculate several keay values\n",
    "maxZ_Acc = np.abs(sequence_df['zBuffer']).max()\n",
    "maxX_Vel = np.abs(sequence_df['xBuffer_quasi_velocity']).max()\n",
    "maxY_Vel = np.abs(sequence_df['yBuffer_quasi_velocity']).max()\n",
    "maxZ_Vel = np.abs(sequence_df['zBuffer_quasi_velocity']).max()\n",
    "minZ_Vel = sequence_df['zBuffer_quasi_velocity'].min()\n",
    "divXZ = maxZ_Vel/maxX_Vel\n",
    "divYZ = maxZ_Vel/maxY_Vel\n",
    "\n",
    "ZImpactReturn = calculate_ZImpactReturn(maxZ_Vel, maxX_Vel, maxY_Vel, minZ_Vel, knock_algo_AngleXLatched)\n",
    "Very_High_Impact = calculate_Very_High_Impact(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], very_high_counter)\n",
    "DeployFlag = calculate_DeployFlag(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], prebuffer_size, knock_event_ended, Acc_Latched_Counter, Vel_Latched_Counter, Acc_Latched_Threshold, Vel_Latched_Threshold)\n",
    "Has_It_Knocked = calculate_Has_It_Knocked(DeployFlag[0], Very_High_Impact, ZImpactReturn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(Has_It_Knocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Label: 0, Detection Result: False\n"
     ]
    }
   ],
   "source": [
    "# Example parameters - replace with actual parameter values as needed\n",
    "params = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': 25,\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': 30,\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': 5,\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': 8500, \n",
    "    'KLOPFALGO_VelThrZLatched_s32': 9000,\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': 8\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "# Test with the first sequence as an example\n",
    "sequence, label = all_sequences[0]\n",
    "result = mimic_knock_detection(sequence, params)\n",
    "\n",
    "print(f\"Sequence Label: {label}, Detection Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8504672897196262\n"
     ]
    }
   ],
   "source": [
    "predictions = [mimic_knock_detection(seq, params) for seq, _ in all_sequences]\n",
    "true_labels = [label for _, label in all_sequences]\n",
    "\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score across 5 folds: 0.8475101418497646\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(all_sequences):  # Use all_sequences directly\n",
    "    # Splitting data into training and test sets for this fold using list comprehension\n",
    "    train = [all_sequences[i] for i in train_index]\n",
    "    test = [all_sequences[i] for i in test_index]\n",
    "    \n",
    "    # Generate predictions for each sequence in the test set\n",
    "    predictions = [mimic_knock_detection(sequence, params) for sequence, _ in test]\n",
    "    \n",
    "    # Extract true labels for the test set\n",
    "    true_labels = [label for _, label in test]\n",
    "    \n",
    "    # Calculate F1 score and append to list\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average F1 score across all folds\n",
    "average_f1 = np.mean(f1_scores)\n",
    "print(f\"Average F1 Score across {k} folds: {average_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETER OPTIMIZATION - STRATEGY: GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.8857109891543178\n",
      "Best Parameters: {'KLOPFALGO_HfaLatchedCounterThr_u8': 15, 'KLOPFALGO_VelLatchedCounterThr_u8': 10, 'KLOPFALGO_VelVeryHighCounterThr_u8': 3, 'KLOPFALGO_HfaThrZLatched_s16': 7500, 'KLOPFALGO_VelThrZLatched_s32': 8000, 'KLOPFALGO_WinkelThrXLatched_s16': 6}\n"
     ]
    }
   ],
   "source": [
    "#Define parameter grid\n",
    "parameter_grid = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': [10, 15, 20, 25],\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': [10, 20, 30, 40],\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': [3,5,8],\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': [6500, 7500, 8500, 9500], \n",
    "    'KLOPFALGO_VelThrZLatched_s32': [6000, 7000, 8000, 9000, 10000],\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': [4,6,8,10]\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = list(itertools.product(*(parameter_grid[param_name] for param_name in parameter_grid)))\n",
    "\n",
    "# Placeholder for best score and corresponding parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for combination in param_combinations:\n",
    "    params = dict(zip(parameter_grid.keys(), combination))\n",
    "    \n",
    "    # List to hold F1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(all_sequences):\n",
    "        train, test = [all_sequences[i] for i in train_index], [all_sequences[i] for i in test_index]\n",
    "        predictions = [mimic_knock_detection(sequence, params) for sequence, _ in test]\n",
    "        true_labels = [label for _, label in test]\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate average F1 score for this parameter combination\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    if avg_f1 > best_score:\n",
    "        best_score = avg_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best F1 Score:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETER OPTIMIZATION - STRATEGY: BAYESIAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | KLOPFA... | KLOPFA... | KLOPFA... | KLOPFA... | KLOPFA... | KLOPFA... |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "knock_detection_cv() got an unexpected keyword argument 'KLOPFALGO_HfaThrZLatched_s16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     39\u001b[0m     f\u001b[38;5;241m=\u001b[39mknock_detection_cv,\n\u001b[1;32m     40\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[1;32m     41\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Use the expected improvement acquisition function to handle exploration-exploitation trade-off\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Output the best parameters and the corresponding score\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest F1 Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/k2oParameter/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/Projects/k2oParameter/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/Projects/k2oParameter/.venv/lib/python3.11/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "\u001b[0;31mTypeError\u001b[0m: knock_detection_cv() got an unexpected keyword argument 'KLOPFALGO_HfaThrZLatched_s16'"
     ]
    }
   ],
   "source": [
    "def knock_detection_cv(KLOPFALGO_HfaLatchedCounterThr_u8, KLOPFALGO_VelLatchedCounterThr_u8, KLOPFALGO_VelVeryHighCounterThr_u8, KLOPFALGO_HfaThrZLatched_s16, KLOPFALGO_VelThrZLatched_s32, KLOPFALGO_WinkelThrXLatched_s16):\n",
    "    # Convert continuous parameters to their appropriate format if necessary\n",
    "    params = {\n",
    "        'KLOPFALGO_HfaLatchedCounterThr_u8': int(KLOPFALGO_HfaLatchedCounterThr_u8),\n",
    "        'KLOPFALGO_VelLatchedCounterThr_u8': int(KLOPFALGO_VelLatchedCounterThr_u8),\n",
    "        'KLOPFALGO_VelVeryHighCounterThr_u8': int(KLOPFALGO_VelVeryHighCounterThr_u8),\n",
    "        'KLOPFALGO_HfaThrZLatched_s16': int(KLOPFALGO_HfaThrZLatched_s16),\n",
    "        'KLOPFALGO_VelThrZLatched_s32': int(KLOPFALGO_VelThrZLatched_s32),\n",
    "        'KLOPFALGO_WinkelThrXLatched_s16': int(KLOPFALGO_WinkelThrXLatched_s16)\n",
    "    }\n",
    "    \n",
    "    f1_scores = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(all_sequences):\n",
    "        train, test = [all_sequences[i] for i in train_index], [all_sequences[i] for i in test_index]\n",
    "        predictions = [mimic_knock_detection(sequence, params) for sequence, _ in test]\n",
    "        true_labels = [label for _, label in test]\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the average F1 score across folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "# Define the parameter bounds\n",
    "pbounds = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': (10, 25),\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': (10, 40),\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': (3, 8),\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': (6000, 10000),\n",
    "    'KLOPFALGO_VelThrZLatched_s32': (6000, 10000),\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': (4, 12)\n",
    "\n",
    "}\n",
    "\n",
    "# Instantiate BayesianOptimization object\n",
    "optimizer = BayesianOptimization(\n",
    "    f=knock_detection_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Use the expected improvement acquisition function to handle exploration-exploitation trade-off\n",
    "optimizer.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "# Output the best parameters and the corresponding score\n",
    "print(\"Best F1 Score:\", optimizer.max['target'])\n",
    "print(\"Best Parameters:\", optimizer.max['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
