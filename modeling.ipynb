{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DEPENDENCIES AND LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import knock_evaluator\n",
    "\n",
    "from knock_evaluator import mimic_knock_detection, find_knock_event_ended, calculate_ZImpactReturn, calculate_Very_High_Impact, calculate_DeployFlag, reshape_sequence, calculate_Has_It_Knocked\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, label):\n",
    "    \"\"\"\n",
    "    Process all files in the given folder and compile data into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing the files.\n",
    "    - label: The label to assign to all data from this folder (e.g., 1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing all processed data from the files.\n",
    "    \"\"\"\n",
    "    all_data = []  # List to hold data from all files\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure we're only processing .csv files\n",
    "        if os.path.isfile(file_path) and file_path.endswith('.csv'):\n",
    "            # Read the file\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Filter rows for xBuffer, yBuffer, zBuffer and reset index\n",
    "            x_data = data[data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "            y_data = data[data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "            z_data = data[data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "            \n",
    "            # Ensure data is aligned\n",
    "            min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "            structured_df = pd.DataFrame({\n",
    "                'x': x_data['Value'].head(min_length),\n",
    "                'y': y_data['Value'].head(min_length),\n",
    "                'z': z_data['Value'].head(min_length),\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            # Append to the list\n",
    "            all_data.append(structured_df)\n",
    "    \n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the valid knock data\n",
    "valid_data_path = 'data/valid/valid1.csv'\n",
    "valid_data = pd.read_csv(valid_data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract only the rows that contain acceleration values and reset index to avoid grouping issues\n",
    "x_data = valid_data[valid_data['Expression'] == 'xBuffer'].reset_index(drop=True)\n",
    "y_data = valid_data[valid_data['Expression'] == 'yBuffer'].reset_index(drop=True)\n",
    "z_data = valid_data[valid_data['Expression'] == 'zBuffer'].reset_index(drop=True)\n",
    "\n",
    "# Ensure we only take as many rows as the shortest among x, y, z to keep data aligned\n",
    "min_length = min(len(x_data), len(y_data), len(z_data))\n",
    "\n",
    "# Reconstruct the DataFrame using the aligned data\n",
    "structured_df_aligned = pd.DataFrame({\n",
    "    'x': x_data['Value'].head(min_length),\n",
    "    'y': y_data['Value'].head(min_length),\n",
    "    'z': z_data['Value'].head(min_length),\n",
    "    'label': 1  # Label for valid knock\n",
    "})\n",
    "\n",
    "structured_df_aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_sequences(df, label):\n",
    "    \"\"\"\n",
    "    Structures the DataFrame such that each 90-row sequence (representing 30 time points of x, y, z data)\n",
    "    is treated as a single observation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the sequences.\n",
    "    - label: The label for these sequences (1 for valid, 0 for invalid).\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tuples, where each tuple is (sequence, label), and\n",
    "      each sequence is a (90, ) shape array if flattening or a (30, 3) array if keeping x, y, z separate.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    num_sequences = len(df) // 90  # Assuming each sequence is exactly 90 rows\n",
    "    \n",
    "    for i in range(num_sequences):\n",
    "        start_idx = i * 90\n",
    "        sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.flatten()  # Flattened sequence\n",
    "        # Alternatively, keep as a (30, 3) array for models that can handle sequence data\n",
    "        # sequence = df.iloc[start_idx:start_idx + 90][['x', 'y', 'z']].values.reshape((30, 3))\n",
    "        sequences.append((sequence, label))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all valid / invalid data into data frames, Label & Combine them to Sequences. Afterwards Combine them to global Sequence structure and Shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_valid = 'data/valid'  \n",
    "folder_path_invalid = 'data/invalid'  \n",
    "valid_data = process_folder(folder_path_valid, label=1)\n",
    "invalid_data = process_folder(folder_path_invalid, label=0)\n",
    "\n",
    "# Assuming valid_data and invalid_data are already loaded and structured with one file per sequence\n",
    "valid_sequences = structure_sequences(valid_data, 1)\n",
    "invalid_sequences = structure_sequences(invalid_data, 0)\n",
    "\n",
    "all_sequences = valid_sequences + invalid_sequences\n",
    "random.shuffle(all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for model training\n",
    "X = np.array([seq[0] for seq in all_sequences])\n",
    "y = np.array([seq[1] for seq in all_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Test Split of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = reshape_sequence(all_sequences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuffer_size = 8\n",
    "knock_event_length_avg = 45\n",
    "knock_event_ended = prebuffer_size + knock_event_length_avg\n",
    "knock_algo_AngleXLatched = 8\n",
    "knock_algo_Vel_Zlatched = 9000\n",
    "knock_algo_Vel_Zlatched_Counter = 20\n",
    "knock_algo_HFA_Zlatched = 3200\n",
    "knock_algo_HFA_Zlatched_Counter = 15\n",
    "ZImpactReturn = False\n",
    "Very_High_Impact = False\n",
    "DeployFlag = False\n",
    "\n",
    "Has_It_Knocked = False\n",
    "\n",
    "very_high_counter = 5\n",
    "Acc_Latched_Counter = 25\n",
    "Acc_Latched_Threshold = 8500\n",
    "Vel_Latched_Counter = 30\n",
    "Vel_Latched_Threshold = 9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the velocities as cumulative sum (integration) of each buffer\n",
    "sequence_df['xBuffer_quasi_velocity'] = np.cumsum(sequence_df['xBuffer'])\n",
    "sequence_df['yBuffer_quasi_velocity'] = np.cumsum(sequence_df['yBuffer'])\n",
    "sequence_df['zBuffer_quasi_velocity'] = np.cumsum(sequence_df['zBuffer'])\n",
    "\n",
    "# Calculate the energies of each buffer\n",
    "sequence_df['xBuffer_quasi_energy'] = (sequence_df['xBuffer_quasi_velocity'])**2\n",
    "sequence_df['yBuffer_quasi_energy'] = (sequence_df['yBuffer_quasi_velocity'])**2\n",
    "sequence_df['zBuffer_quasi_energy'] = (sequence_df['zBuffer_quasi_velocity'])**2\n",
    "\n",
    "sequence_df['xBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['xBuffer_quasi_velocity']))\n",
    "sequence_df['yBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['yBuffer_quasi_velocity']))\n",
    "sequence_df['zBuffer_quasi_work'] = np.cumsum(np.abs(sequence_df['zBuffer_quasi_velocity']))\n",
    "\n",
    "knock_event_ended_dynamic = find_knock_event_ended(sequence_df['zBuffer'], knock_algo_HFA_Zlatched, knock_algo_HFA_Zlatched_Counter, prebuffer_size)\n",
    "\n",
    "# Nulling values after the knock_event_ended parameter by setting them to 0\n",
    "# Update based on the dynamically determined knock_event_ended value\n",
    "if knock_event_ended_dynamic is not None and knock_event_ended_dynamic != 0:\n",
    "    knock_event_ended = knock_event_ended_dynamic\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended_dynamic+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "else:\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_velocity'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_energy'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'xBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'yBuffer_quasi_work'] = 0\n",
    "    sequence_df.loc[knock_event_ended+1:, 'zBuffer_quasi_work'] = 0\n",
    "\n",
    "\n",
    "#Calculate several keay values\n",
    "maxZ_Acc = np.abs(sequence_df['zBuffer']).max()\n",
    "maxX_Vel = np.abs(sequence_df['xBuffer_quasi_velocity']).max()\n",
    "maxY_Vel = np.abs(sequence_df['yBuffer_quasi_velocity']).max()\n",
    "maxZ_Vel = np.abs(sequence_df['zBuffer_quasi_velocity']).max()\n",
    "minZ_Vel = sequence_df['zBuffer_quasi_velocity'].min()\n",
    "divXZ = maxZ_Vel/maxX_Vel\n",
    "divYZ = maxZ_Vel/maxY_Vel\n",
    "\n",
    "ZImpactReturn = calculate_ZImpactReturn(maxZ_Vel, maxX_Vel, maxY_Vel, minZ_Vel, knock_algo_AngleXLatched)\n",
    "Very_High_Impact = calculate_Very_High_Impact(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], very_high_counter)\n",
    "DeployFlag = calculate_DeployFlag(sequence_df['zBuffer'], sequence_df['zBuffer_quasi_velocity'], prebuffer_size, knock_event_ended, Acc_Latched_Counter, Vel_Latched_Counter, Acc_Latched_Threshold, Vel_Latched_Threshold)\n",
    "Has_It_Knocked = calculate_Has_It_Knocked(DeployFlag[0], Very_High_Impact, ZImpactReturn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Has_It_Knocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters - replace with actual parameter values as needed\n",
    "params = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': 25,\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': 30,\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': 5,\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': 8500, \n",
    "    'KLOPFALGO_VelThrZLatched_s32': 9000,\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': 8\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "# Test with the first sequence as an example\n",
    "sequence, label = all_sequences[0]\n",
    "result = mimic_knock_detection(sequence, params)\n",
    "\n",
    "print(f\"Sequence Label: {label}, Detection Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [mimic_knock_detection(seq, params) for seq, _ in all_sequences]\n",
    "true_labels = [label for _, label in all_sequences]\n",
    "\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(all_sequences):  # Use all_sequences directly\n",
    "    # Splitting data into training and test sets for this fold using list comprehension\n",
    "    train = [all_sequences[i] for i in train_index]\n",
    "    test = [all_sequences[i] for i in test_index]\n",
    "    \n",
    "    # Generate predictions for each sequence in the test set\n",
    "    predictions = [mimic_knock_detection(sequence, params) for sequence, _ in test]\n",
    "    \n",
    "    # Extract true labels for the test set\n",
    "    true_labels = [label for _, label in test]\n",
    "    \n",
    "    # Calculate F1 score and append to list\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average F1 score across all folds\n",
    "average_f1 = np.mean(f1_scores)\n",
    "print(f\"Average F1 Score across {k} folds: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameter grid\n",
    "parameter_grid = {\n",
    "    'KLOPFALGO_HfaLatchedCounterThr_u8': [10, 15, 20, 25],\n",
    "    'KLOPFALGO_VelLatchedCounterThr_u8': [10, 20, 30, 40],\n",
    "    'KLOPFALGO_VelVeryHighCounterThr_u8': [3,5,8],\n",
    "    'KLOPFALGO_HfaThrZLatched_s16': [6500, 7500, 8500, 9500], \n",
    "    'KLOPFALGO_VelThrZLatched_s32': [6000, 7000, 8000, 9000, 10000],\n",
    "    'KLOPFALGO_WinkelThrXLatched_s16': [4,6,8,10]\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = list(itertools.product(*(parameter_grid[param_name] for param_name in parameter_grid)))\n",
    "\n",
    "# Placeholder for best score and corresponding parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for combination in param_combinations:\n",
    "    params = dict(zip(parameter_grid.keys(), combination))\n",
    "    \n",
    "    # List to hold F1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(all_sequences):\n",
    "        train, test = [all_sequences[i] for i in train_index], [all_sequences[i] for i in test_index]\n",
    "        predictions = [mimic_knock_detection(sequence, params) for sequence, _ in test]\n",
    "        true_labels = [label for _, label in test]\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate average F1 score for this parameter combination\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    if avg_f1 > best_score:\n",
    "        best_score = avg_f1\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best F1 Score:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
